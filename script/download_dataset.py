from _pytest._py.path import local
from _pytest.cacheprovider import cache
from torch._inductor.lowering import rev
from torch.ao.ns.fx.n_shadows_utils import SHADOW_WRAPPER_NODE_NAME_PREFIX
from torch.__config__ import show
from torch import save
import os
import modelscope.hub.snapshot_download as MSDownload
from pathlib import Path
import huggingface_hub as HFH
import rapid_attention as RA


os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"


def download_wikipedia_dataset(datasets_dir: Path):
    datasets_dir.mkdir(exist_ok=True)
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½ä¸­æ–‡ç»´åŸºæ•°æ®é›†...")
    try:
        download_path = HFH.snapshot_download(
            repo_id="wikimedia/wikipedia",  # æ•°æ®é›†ä»“åº“ID
            repo_type="dataset",  # æ˜ç¡®æ˜¯æ•°æ®é›†ï¼ˆä¸æ˜¯æ¨¡å‹ï¼‰
            revision="main",  # åˆ†æ”¯
            allow_patterns="20231101.zh/",  # åªä¸‹è½½ä¸­æ–‡ç»´åŸºç›®å½•ï¼Œé¿å…ä¸‹å…¶ä»–è¯­è¨€
            local_dir=datasets_dir / "zh_wiki",  # æœ¬åœ°ä¿å­˜ç›®å½•
        )

    except Exception as e:
        print(f"âŒ ä¸‹è½½å‡ºé”™ï¼š{str(e)}")


def download_deepctrl_sft_dataset(datasets_dir: Path):
    datasets_dir.mkdir(exist_ok=True)
    dataset_name = "deepctrl/deepctrl-sft-data"
    save_dir = datasets_dir / dataset_name
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›† {dataset_name} ...")
    try:
        # https://github.com/modelscope/modelscope/blob/master/modelscope/hub/snapshot_download.py#L155
        download_path = MSDownload.dataset_snapshot_download(
            dataset_id=dataset_name,
            local_dir=save_dir,
        )

    except Exception as e:
        print(f"âŒ modelscope ä¸‹è½½æ•°æ®é›† {dataset_name} å‡ºé”™ï¼š{str(e)}")

def check_login_huggingface():
    from huggingface_hub import login
    hf_token = os.getenv("HF_TOKEN")
    if hf_token is None:
        raise ValueError("ç¯å¢ƒå˜é‡ HF_TOKEN æœªè®¾ç½®ï¼Œè¯·è®¾ç½®åé‡è¯•ã€‚")
    login(token=hf_token)
    print("âœ… å·²æˆåŠŸç™»å½• Hugging Face Hubã€‚")

def main():
    project_root = RA.utils.common.find_project_root()
    dataset_dir = project_root / "datasets"
    print(f"âœ… é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"âœ… æ•°æ®é›†ç›®å½•: {dataset_dir}")
    check_login_huggingface()
    download_wikipedia_dataset(dataset_dir)
    download_deepctrl_sft_dataset(dataset_dir)
    print("âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")


if __name__ == "__main__":
    main()